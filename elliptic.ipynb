{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in and pre-process data\n",
    "columns = ['txId', 'timestep']\n",
    "\n",
    "class_names = ['Illicit', 'Legal']\n",
    "\n",
    "# Name the columns without known names\n",
    "for x in range(165) :\n",
    "    columns.append('col'+ str(x))\n",
    "\n",
    "# These are the output labels\n",
    "# classes = pd.read_csv('/kaggle/input/elliptic-data-set/elliptic_bitcoin_dataset/elliptic_txs_classes.csv')\n",
    "classes = pd.read_csv('elliptic_bitcoin_dataset/elliptic_txs_classes.csv')\n",
    "\n",
    "# These are 167 columns of feature data\n",
    "# features = pd.read_csv('/kaggle/input/elliptic-data-set/elliptic_bitcoin_dataset/elliptic_txs_features.csv', names=columns)\n",
    "features = pd.read_csv('elliptic_bitcoin_dataset/elliptic_txs_features.csv', names=columns)\n",
    "\n",
    "# Flatten the data, append class to features\n",
    "data = features.assign(result=classes['class'])\n",
    "\n",
    "# Trim the data to include only labeled data. \n",
    "dataset = data[data['result'] != \"unknown\"]\n",
    "dataset['result'] = pd.to_numeric(dataset.result) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 means illicit and 1 means licit\n",
    "dataset['result'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0.dev20230205\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, ChebConv  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m graph_data\u001b[39m=\u001b[39mdataset\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "graph_data=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_train = len(graph_data)\n",
    "num_test= len(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, lrate):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16, cached=True,\n",
    "                             normalize=not use_gdc)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes, cached=True,\n",
    "                             normalize=not use_gdc)\n",
    "        #self.optimizer = torch.optim.Adam(self.parameters(), lr=lrate, weight_decay=0.0005)\n",
    "        #self.optimizer = torch.optim.Adam([\n",
    "        #    dict(params=self.conv1.parameters(),weight_decay=5e-4),\n",
    "        #    dict(params=self.conv2.parameters(),weight_decay=0)\n",
    "        #], lr=lrate)\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=lrate)\n",
    "        # self.conv1 = ChebConv(data.num_features, 16, K=2)\n",
    "        # self.conv2 = ChebConv(16, data.num_features, K=2)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index, edge_weight = graph_data.x, graph_data.edge_index, graph_data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def evaluate_proposal(self, data, w=None):\n",
    "        self.los =0\n",
    "        if w is not None:\n",
    "            self.loadparameters(w)\n",
    "        if (data=='train'):\n",
    "            prob = copy.deepcopy(self.forward().detach())\n",
    "            for _, mask in graph_data('train_mask'):\n",
    "                y_pred = prob[mask].max(1)[1]\n",
    "            loss = F.nll_loss(self.forward()[graph_data.train_mask], graph_data.y[graph_data.train_mask])\n",
    "            self.los += loss\n",
    "            prob = prob[mask]\n",
    "        else:\n",
    "            prob = copy.deepcopy(self.forward().detach())\n",
    "            for _, mask in graph_data('test_mask'):\n",
    "                y_pred = prob[mask].max(1)[1]\n",
    "            loss = F.nll_loss(self.forward()[graph_data.test_mask], graph_data.y[graph_data.test_mask])\n",
    "            self.los += loss\n",
    "            prob = prob[mask]\n",
    "        return y_pred, prob\n",
    "\n",
    "    def langevin_gradient(self, w):\n",
    "\n",
    "        self.loadparameters(w)\n",
    "        self.los = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.nll_loss(self.forward()[graph_data.train_mask], graph_data.y[graph_data.train_mask])\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.los += copy.deepcopy(loss.item())\n",
    "        return copy.deepcopy(self.state_dict())\n",
    "\n",
    "    def getparameters(self, w=None):\n",
    "        l = np.array([1, 2])\n",
    "        dic = {}\n",
    "        if w is None:\n",
    "            dic = self.state_dict()\n",
    "        else:\n",
    "            dic = copy.deepcopy(w)\n",
    "        for name in sorted(dic.keys()):\n",
    "            l = np.concatenate((l, np.array(copy.deepcopy(dic[name])).reshape(-1)), axis=None)\n",
    "        l = l[2:]\n",
    "        return l\n",
    "\n",
    "    def dictfromlist(self, param):\n",
    "        dic = {}\n",
    "        i = 0\n",
    "        for name in sorted(self.state_dict().keys()):\n",
    "            dic[name] = torch.FloatTensor(param[i:i + (self.state_dict()[name]).view(-1).shape[0]]).view(\n",
    "                self.state_dict()[name].shape)\n",
    "            i += (self.state_dict()[name]).view(-1).shape[0]\n",
    "        # self.loadparameters(dic)\n",
    "        return dic\n",
    "\n",
    "    def loadparameters(self, param):\n",
    "        self.load_state_dict(param)\n",
    "\n",
    "    def addnoiseandcopy(self, w, mea, std_dev):\n",
    "        dic = {}\n",
    "        #w = self.state_dict()\n",
    "        for name in (w.keys()):\n",
    "            dic[name] = copy.deepcopy(w[name]) + torch.zeros(w[name].size()).normal_(mean=mea, std=std_dev)\n",
    "        self.loadparameters(dic)\n",
    "        return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "class ptReplica(multiprocessing.Process):\n",
    "    def __init__(self, use_langevin_gradients, learn_rate, w, minlim_param, maxlim_param, samples,\n",
    "                 burn_in, temperature, swap_interval, path, parameter_queue, main_process, event, step_size):\n",
    "        self.gnn = Net(learn_rate)\n",
    "        multiprocessing.Process.__init__(self)\n",
    "        self.processID = temperature\n",
    "        self.parameter_queue = parameter_queue\n",
    "        self.signal_main = main_process\n",
    "        self.event = event\n",
    "        self.temperature = temperature\n",
    "        self.adapttemp = temperature\n",
    "        self.swap_interval = swap_interval\n",
    "        self.path = path\n",
    "        self.burn_in = burn_in\n",
    "        self.samples = samples\n",
    "        self.traindata = 'train'\n",
    "        self.testdata = 'test'\n",
    "        self.w = w\n",
    "        self.minY = np.zeros((1, 1))\n",
    "        self.maxY = np.zeros((1, 1))\n",
    "        self.minlim_param = minlim_param\n",
    "        self.maxlim_param = maxlim_param\n",
    "        self.use_langevin_gradients = use_langevin_gradients\n",
    "        self.sgd_depth = 1  # Keep as 1\n",
    "        self.learn_rate = learn_rate\n",
    "        self.l_prob = 0.6  # Ratio of langevin based proposals, higher value leads to more computation time, evaluate for different problems\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def rmse(self, predictions, targets):\n",
    "        return self.gnn.los.item()\n",
    "\n",
    "    @staticmethod\n",
    "    def likelihood_func(gnn, data, w, temp):\n",
    "        if w is not None:\n",
    "            fx, prob = gnn.evaluate_proposal(data, w)\n",
    "        else:\n",
    "            fx, prob = gnn.evaluate_proposal(data)\n",
    "\n",
    "        if (data == 'train'):\n",
    "            y = graph_data.y[graph_data.train_mask]\n",
    "            rmse = gnn.los / num_train\n",
    "            lhood = 0\n",
    "            for i in range(num_train):\n",
    "                for k in range(dataset.num_classes):\n",
    "                    if k == y[i]:\n",
    "                        if prob[i, k] == 0:\n",
    "                            lhood+=0\n",
    "                        else:\n",
    "                            lhood += (prob[i,k])\n",
    "        else:\n",
    "            y = graph_data.y[graph_data.test_mask]\n",
    "            rmse = gnn.los / num_test\n",
    "            lhood = 0\n",
    "            for i in range(num_test):\n",
    "                for k in range(dataset.num_classes):\n",
    "                    if k == y[i]:\n",
    "                        if prob[i, k] == 0:\n",
    "                            lhood += 0\n",
    "                        else:\n",
    "                            lhood += (prob[i, k])\n",
    "\n",
    "        return [lhood / temp , fx, rmse]\n",
    "\n",
    "    def prior_likelihood(self, sigma_squared, w_list):\n",
    "        part1 = -1 * ((len(w_list)) / 2) * np.log(sigma_squared)\n",
    "        part2 = 1 / (2 * sigma_squared) * (sum(np.square(w_list)))\n",
    "        log_loss = part1 - part2\n",
    "        return log_loss\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        gnn =self.gnn\n",
    "        if (data == 'train'):\n",
    "            prob = copy.deepcopy(gnn().detach())\n",
    "            for _, mask in graph_data('train_mask'):\n",
    "                pred = prob[mask].max(1)[1]\n",
    "                acc = pred.eq(graph_data.y[mask]).sum().item() / mask.sum().item()\n",
    "        else:\n",
    "            prob = copy.deepcopy(gnn().detach())\n",
    "            for _, mask in graph_data('test_mask'):\n",
    "                pred = prob[mask].max(1)[1]\n",
    "                acc = pred.eq(graph_data.y[mask]).sum().item() / mask.sum().item()\n",
    "        return 100 * acc\n",
    "\n",
    "    def run(self):\n",
    "        samples = self.samples\n",
    "        gnn = self.gnn\n",
    "\n",
    "        # Random Initialisation of weights\n",
    "        w = gnn.state_dict()\n",
    "        w_size = len(gnn.getparameters(w))\n",
    "        step_w = self.step_size\n",
    "\n",
    "        rmse_train = np.zeros(samples)\n",
    "        rmse_test = np.zeros(samples)\n",
    "        acc_train = np.zeros(samples)\n",
    "        acc_test = np.zeros(samples)\n",
    "        likelihood_proposal_array = np.zeros(samples)\n",
    "        likelihood_array = np.zeros(samples)\n",
    "        diff_likelihood_array = np.zeros(samples)\n",
    "        weight_array = np.zeros(samples)\n",
    "        weight_array1 = np.zeros(samples)\n",
    "        weight_array2 = np.zeros(samples)\n",
    "        weight_array3 = np.zeros(samples)\n",
    "        weight_array4 = np.zeros(samples)\n",
    "        sum_value_array = np.zeros(samples)\n",
    "\n",
    "        w_proposal_ = np.random.randn(w_size)\n",
    "        w_proposal = gnn.dictfromlist(w_proposal_)\n",
    "        train = 'train'\n",
    "        test = 'test'\n",
    "\n",
    "        sigma_squared = 25\n",
    "        prior_current = self.prior_likelihood(sigma_squared, w_proposal_)\n",
    "\n",
    "        eta = 0 #junk\n",
    "\n",
    "        [likelihood, pred_train, rmsetrain] = self.likelihood_func(gnn, train, w_proposal, self.adapttemp)\n",
    "        [likelihood, pred_train, rmsetest] = self.likelihood_func(gnn, test, w_proposal, self.adapttemp)\n",
    "\n",
    "        num_accepted = 0\n",
    "        langevin_count = 0\n",
    "        pt_samples = samples * pt_percentage # PT in canonical form with adaptive temp will work till assigned limit\n",
    "        init_count = 0\n",
    "\n",
    "        rmse_train[0] = rmsetrain\n",
    "        rmse_test[0] = rmsetest\n",
    "        acc_train[0] = self.accuracy(train)\n",
    "        acc_test[0] = self.accuracy(test)\n",
    "\n",
    "        likelihood_proposal_array[0] = 0\n",
    "        likelihood_array[0] = 0\n",
    "        diff_likelihood_array[0] = 0\n",
    "        weight_array[0] = 0\n",
    "        weight_array1[0] = 0\n",
    "        weight_array2[0] = 0\n",
    "        weight_array3[0] = 0\n",
    "        weight_array4[0] = 0\n",
    "\n",
    "        sum_value_array[0] = 0\n",
    "\n",
    "        for i in range(\n",
    "                samples):  # Begin sampling --------------------------------------------------------------------------\n",
    "\n",
    "            if i < pt_samples:\n",
    "                self.adapttemp = self.temperature  # T1=T/log(k+1);\n",
    "            if i == pt_samples and init_count == 0:  # Move to canonical MCMC\n",
    "                self.adapttemp = 1\n",
    "                [likelihood, pred_train, rmsetrain] = self.likelihood_func(gnn, train, w_proposal, self.adapttemp)\n",
    "                [_, pred_test, rmsetest] = self.likelihood_func(gnn, test, w_proposal, self.adapttemp)\n",
    "                init_count = 1\n",
    "\n",
    "            lx = np.random.uniform(0, 1, 1)\n",
    "            old_w = gnn.state_dict()\n",
    "\n",
    "            if (self.use_langevin_gradients is True) and (lx < self.l_prob):\n",
    "                w_gd = gnn.langevin_gradient(copy.deepcopy(w))  # Eq 8\n",
    "                w_proposal = gnn.addnoiseandcopy(w_gd, 0, step_w)  # np.random.normal(w_gd, step_w, w_size) # Eq 7\n",
    "                w_prop_gd = gnn.langevin_gradient(copy.deepcopy(w_proposal))\n",
    "                wc_delta = (gnn.getparameters(copy.deepcopy(w)) - gnn.getparameters(w_prop_gd))\n",
    "                wp_delta = (gnn.getparameters(w_proposal) - gnn.getparameters(w_gd))\n",
    "                sigma_sq = step_w * step_w\n",
    "                first = -0.5 * np.sum(wc_delta * wc_delta) / sigma_sq  # this is wc_delta.T  *  wc_delta /sigma_sq\n",
    "                second = -0.5 * np.sum(wp_delta * wp_delta) / sigma_sq\n",
    "                diff_prop = first - second\n",
    "                diff_prop = diff_prop\n",
    "                langevin_count = langevin_count + 1\n",
    "            else:\n",
    "                diff_prop = 0\n",
    "                w_proposal = gnn.addnoiseandcopy(copy.deepcopy(w), 0, step_w)  # np.random.normal(w, step_w, w_size)\n",
    "\n",
    "            [likelihood_proposal, pred_train, rmsetrain] = self.likelihood_func(gnn, train, copy.deepcopy(w_proposal), self.adapttemp)\n",
    "            [likelihood_ignore, pred_test, rmsetest] = self.likelihood_func(gnn, test, copy.deepcopy(w_proposal), self.adapttemp)\n",
    "\n",
    "            prior_prop = self.prior_likelihood(sigma_squared,\n",
    "                                               gnn.getparameters(copy.deepcopy(w_proposal)))  # takes care of the gradients\n",
    "            diff_likelihood = likelihood_proposal - likelihood\n",
    "            diff_prior = prior_prop - prior_current\n",
    "\n",
    "            likelihood_proposal_array[i] = likelihood_proposal\n",
    "            likelihood_array[i] = likelihood\n",
    "            diff_likelihood_array[i] = diff_likelihood\n",
    "\n",
    "            sum_value = diff_likelihood + diff_prior + diff_prop\n",
    "            sum_value_array[i] = sum_value\n",
    "            u = np.log(random.uniform(0, 1))\n",
    "\n",
    "            if u < sum_value:\n",
    "                num_accepted = num_accepted + 1\n",
    "                likelihood = likelihood_proposal\n",
    "                prior_current = prior_prop\n",
    "                w = copy.deepcopy(w_proposal)  # rnn.getparameters(w_proposal)\n",
    "                acc_train1 = self.accuracy(train)\n",
    "                acc_test1 = self.accuracy(test)\n",
    "                print (i, rmsetrain, rmsetest, acc_train1, acc_test1, 'accepted')\n",
    "                rmse_train[i] = rmsetrain\n",
    "                rmse_test[i] = rmsetest\n",
    "                acc_train[i,] = acc_train1\n",
    "                acc_test[i,] = acc_test1\n",
    "\n",
    "            else:\n",
    "                w = old_w\n",
    "                gnn.loadparameters(w)\n",
    "                acc_train1 = self.accuracy(train)\n",
    "                acc_test1 = self.accuracy(test)\n",
    "                print (i, rmsetrain, rmsetest, acc_train1, acc_test1, 'rejected')\n",
    "                rmse_train[i,] = rmse_train[i - 1,]\n",
    "                rmse_test[i,] = rmse_test[i - 1,]\n",
    "                acc_train[i,] = acc_train[i - 1,]\n",
    "                acc_test[i,] = acc_test[i - 1,]\n",
    "\n",
    "            ll = gnn.getparameters()\n",
    "            #print(ll.size)\n",
    "            weight_array[i] = ll[0]\n",
    "            weight_array1[i] = ll[100]\n",
    "            weight_array2[i] = ll[1000]\n",
    "            weight_array3[i] = ll[5000]\n",
    "            weight_array4[i] = ll[8000]\n",
    "\n",
    "            if (i + 1) % self.swap_interval == 0:\n",
    "                param = np.concatenate([np.asarray([gnn.getparameters(w)]).reshape(-1), np.asarray([eta]).reshape(-1),\n",
    "                                        np.asarray([likelihood]), np.asarray([self.adapttemp]), np.asarray([i])])\n",
    "                self.parameter_queue.put(param)\n",
    "                self.signal_main.set()\n",
    "                self.event.clear()\n",
    "                self.event.wait()\n",
    "                result = self.parameter_queue.get()\n",
    "                w = gnn.dictfromlist(result[0:w_size])\n",
    "                eta = result[w_size]\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(i, rmsetrain, rmsetest, 'Iteration Number and MAE Train & Test')\n",
    "\n",
    "        param = np.concatenate(\n",
    "            [np.asarray([gnn.getparameters(w)]).reshape(-1), np.asarray([eta]).reshape(-1), np.asarray([likelihood]),\n",
    "             np.asarray([self.adapttemp]), np.asarray([i])])\n",
    "\n",
    "        self.signal_main.set()\n",
    "\n",
    "        print((num_accepted * 100 / (samples * 1.0)), '% was Accepted')\n",
    "        accept_ratio = num_accepted / (samples * 1.0) * 100\n",
    "\n",
    "        print((langevin_count * 100 / (samples * 1.0)), '% was Langevin')\n",
    "        langevin_ratio = langevin_count / (samples * 1.0) * 100\n",
    "\n",
    "        print('Exiting the Thread', self.temperature)\n",
    "\n",
    "        file_name = self.path + '/predictions/sum_value_' + str(self.temperature) + '.txt'\n",
    "        np.savetxt(file_name, sum_value_array, fmt='%1.2f')\n",
    "\n",
    "        file_name = self.path + '/predictions/weight[0]_' + str(self.temperature) + '.txt'\n",
    "        np.savetxt(file_name, weight_array, fmt='%1.2f')\n",
    "\n",
    "        file_name = self.path + '/predictions/weight[100]_' + str(self.temperature) + '.txt'\n",
    "        np.savetxt(file_name, weight_array1, fmt='%1.2f')\n",
    "\n",
    "        file_name = self.path + '/predictions/weight[1000]_' + str(self.temperature) + '.txt'\n",
    "        np.savetxt(file_name, weight_array2, fmt='%1.2f')\n",
    "\n",
    "        file_name = self.path + '/predictions/weight[5000]_' + str(self.temperature) + '.txt'\n",
    "        np.savetxt(file_name, weight_array3, fmt='%1.2f')\n",
    "\n",
    "        file_name = self.path + '/predictions/weight[8000]_' + str(self.temperature) + '.txt'\n",
    "        np.savetxt(file_name, weight_array4, fmt='%1.2f')\n",
    "\n",
    "        file_name = self.path + '/predictions/rmse_test_chain_' + str(self.temperature) + '.txt'\n",
    "        np.savetxt(file_name, rmse_test, fmt='%1.2f')\n",
    "\n",
    "        file_name = self.path + '/predictions/rmse_train_chain_' + str(self.temperature) + '.txt'\n",
    "        np.savetxt(file_name, rmse_train, fmt='%1.2f')\n",
    "\n",
    "        file_name = self.path + '/predictions/acc_test_chain_' + str(self.temperature) + '.txt'\n",
    "        np.savetxt(file_name, acc_test, fmt='%1.2f')\n",
    "\n",
    "        file_name = self.path + '/predictions/acc_train_chain_' + str(self.temperature) + '.txt'\n",
    "        np.savetxt(file_name, acc_train, fmt='%1.2f')\n",
    "\n",
    "        file_name = self.path + '/predictions/accept_percentage' + str(self.temperature) + '.txt'\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write('%d' % accept_ratio)\n",
    "\n",
    "        file_name = self.path + '/likelihood_value_' + str(self.temperature) + '.txt'\n",
    "        np.savetxt(file_name, likelihood_array, fmt='%1.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Manages the parallel tempering, initialises and executes the parallel chains\n",
    "\n",
    "class ParallelTempering:\n",
    "    def __init__(self, use_langevin_gradients, learn_rate, num_chains, maxtemp, NumSample, swap_interval,\n",
    "                 path, bi, step_size):\n",
    "        gnn = Net(learn_rate)\n",
    "        self.gnn = gnn\n",
    "        self.traindata = 'train'\n",
    "        self.testdata = 'test'\n",
    "        self.num_param = len(gnn.getparameters(\n",
    "            gnn.state_dict()))  # (topology[0] * topology[1]) + (topology[1] * topology[2]) + topology[1] + topology[2]\n",
    "        # Parallel Tempering variables\n",
    "        self.swap_interval = swap_interval\n",
    "        self.path = path\n",
    "        self.maxtemp = maxtemp\n",
    "        self.num_swap = 0\n",
    "        self.total_swap_proposals = 0\n",
    "        self.num_chains = num_chains\n",
    "        self.chains = []\n",
    "        self.temperatures = []\n",
    "        self.NumSamples = int(NumSample / self.num_chains)\n",
    "        self.sub_sample_size = max(1, int(0.05 * self.NumSamples))\n",
    "        # create queues for transfer of parameters between process chain\n",
    "        self.parameter_queue = [multiprocessing.Queue() for i in range(num_chains)]\n",
    "        self.chain_queue = multiprocessing.JoinableQueue()\n",
    "        self.wait_chain = [multiprocessing.Event() for i in range(self.num_chains)]\n",
    "        self.event = [multiprocessing.Event() for i in range(self.num_chains)]\n",
    "        self.all_param = None\n",
    "        self.geometric = True  # True (geometric)  False (Linear)\n",
    "        self.minlim_param = 0.0\n",
    "        self.maxlim_param = 0.0\n",
    "        self.minY = np.zeros((1, 1))\n",
    "        self.maxY = np.ones((1, 1))\n",
    "        self.model_signature = 0.0\n",
    "        self.learn_rate = learn_rate\n",
    "        self.use_langevin_gradients = use_langevin_gradients\n",
    "        self.masternumsample = NumSample\n",
    "        self.burni = bi\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def default_beta_ladder(self, ndim, ntemps,\n",
    "                            Tmax):  # https://github.com/konqr/ptemcee/blob/master/ptemcee/sampler.py\n",
    "        \"\"\"\n",
    "        Returns a ladder of :math:`\\beta \\equiv 1/T` under a geometric spacing that is determined by the\n",
    "        arguments ``ntemps`` and ``Tmax``.  The temperature selection algorithm works as follows:\n",
    "        Ideally, ``Tmax`` should be specified such that the tempered posterior looks like the prior at\n",
    "        this temperature.  If using adaptive parallel tempering, per `arXiv:1501.05823\n",
    "        <http://arxiv.org/abs/1501.05823>`_, choosing ``Tmax = inf`` is a safe bet, so long as\n",
    "        ``ntemps`` is also specified.\n",
    "        \"\"\"\n",
    "        if type(ndim) != int or ndim < 1:\n",
    "            raise ValueError('Invalid number of dimensions specified.')\n",
    "        if ntemps is None and Tmax is None:\n",
    "            raise ValueError('Must specify one of ``ntemps`` and ``Tmax``.')\n",
    "        if Tmax is not None and Tmax <= 1:\n",
    "            raise ValueError('``Tmax`` must be greater than 1.')\n",
    "        if ntemps is not None and (type(ntemps) != int or ntemps < 1):\n",
    "            raise ValueError('Invalid number of temperatures specified.')\n",
    "\n",
    "        tstep = np.array([25.2741, 7., 4.47502, 3.5236, 3.0232,\n",
    "                          2.71225, 2.49879, 2.34226, 2.22198, 2.12628,\n",
    "                          2.04807, 1.98276, 1.92728, 1.87946, 1.83774,\n",
    "                          1.80096, 1.76826, 1.73895, 1.7125, 1.68849,\n",
    "                          1.66657, 1.64647, 1.62795, 1.61083, 1.59494,\n",
    "                          1.58014, 1.56632, 1.55338, 1.54123, 1.5298,\n",
    "                          1.51901, 1.50881, 1.49916, 1.49, 1.4813,\n",
    "                          1.47302, 1.46512, 1.45759, 1.45039, 1.4435,\n",
    "                          1.4369, 1.43056, 1.42448, 1.41864, 1.41302,\n",
    "                          1.40761, 1.40239, 1.39736, 1.3925, 1.38781,\n",
    "                          1.38327, 1.37888, 1.37463, 1.37051, 1.36652,\n",
    "                          1.36265, 1.35889, 1.35524, 1.3517, 1.34825,\n",
    "                          1.3449, 1.34164, 1.33847, 1.33538, 1.33236,\n",
    "                          1.32943, 1.32656, 1.32377, 1.32104, 1.31838,\n",
    "                          1.31578, 1.31325, 1.31076, 1.30834, 1.30596,\n",
    "                          1.30364, 1.30137, 1.29915, 1.29697, 1.29484,\n",
    "                          1.29275, 1.29071, 1.2887, 1.28673, 1.2848,\n",
    "                          1.28291, 1.28106, 1.27923, 1.27745, 1.27569,\n",
    "                          1.27397, 1.27227, 1.27061, 1.26898, 1.26737,\n",
    "                          1.26579, 1.26424, 1.26271, 1.26121,\n",
    "                          1.25973])\n",
    "\n",
    "        if ndim > tstep.shape[0]:\n",
    "            # An approximation to the temperature step at large\n",
    "            # dimension\n",
    "            tstep = 1.0 + 2.0 * np.sqrt(np.log(4.0)) / np.sqrt(ndim)\n",
    "        else:\n",
    "            tstep = tstep[ndim - 1]\n",
    "\n",
    "        appendInf = False\n",
    "        if Tmax == np.inf:\n",
    "            appendInf = True\n",
    "            Tmax = None\n",
    "            ntemps = ntemps - 1\n",
    "\n",
    "        if ntemps is not None:\n",
    "            if Tmax is None:\n",
    "                # Determine Tmax from ntemps.\n",
    "                Tmax = tstep ** (ntemps - 1)\n",
    "        else:\n",
    "            if Tmax is None:\n",
    "                raise ValueError('Must specify at least one of ``ntemps'' and '\n",
    "                                 'finite ``Tmax``.')\n",
    "\n",
    "            # Determine ntemps from Tmax.\n",
    "            ntemps = int(np.log(Tmax) / np.log(tstep) + 2)\n",
    "\n",
    "        betas = np.logspace(0, -np.log10(Tmax), ntemps)\n",
    "        if appendInf:\n",
    "            # Use a geometric spacing, but replace the top-most temperature with\n",
    "            # infinity.\n",
    "            betas = np.concatenate((betas, [0]))\n",
    "\n",
    "        return betas\n",
    "\n",
    "    def assign_temperatures(self):\n",
    "        if self.geometric == True:\n",
    "            betas = self.default_beta_ladder(2, ntemps=self.num_chains, Tmax=self.maxtemp)\n",
    "            for i in range(0, self.num_chains):\n",
    "                self.temperatures.append(np.inf if betas[i] == 0 else 1.0 / betas[i])\n",
    "                # print (self.temperatures[i])\n",
    "        else:\n",
    "\n",
    "            tmpr_rate = (self.maxtemp / self.num_chains)\n",
    "            temp = 1\n",
    "            for i in range(0, self.num_chains):\n",
    "                self.temperatures.append(temp)\n",
    "                temp += tmpr_rate\n",
    "\n",
    "    def initialize_chains(self, burn_in):\n",
    "        self.burn_in = burn_in\n",
    "        self.assign_temperatures()\n",
    "        self.minlim_param = np.repeat([-100], self.num_param)  # priors for nn weights\n",
    "        self.maxlim_param = np.repeat([100], self.num_param)\n",
    "        for i in range(0, self.num_chains):\n",
    "            w = np.random.randn(self.num_param)\n",
    "            w = self.gnn.dictfromlist(w)\n",
    "            self.chains.append(\n",
    "                ptReplica(self.use_langevin_gradients, self.learn_rate, w, self.minlim_param, self.maxlim_param,\n",
    "                          self.NumSamples, self.burn_in, self.temperatures[i], self.swap_interval, self.path,\n",
    "                          self.parameter_queue[i], self.wait_chain[i], self.event[i], self.step_size))\n",
    "\n",
    "    def surr_procedure(self, queue):\n",
    "        if queue.empty() is False:\n",
    "            return queue.get()\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    def swap_procedure(self, parameter_queue_1, parameter_queue_2):\n",
    "        #        if parameter_queue_2.empty() is False and parameter_queue_1.empty() is False:\n",
    "        param1 = parameter_queue_1.get()\n",
    "        param2 = parameter_queue_2.get()\n",
    "        w1 = param1[0:self.num_param]\n",
    "        w1 = self.gnn.dictfromlist(w1)\n",
    "        T1 = param1[self.num_param + 2]\n",
    "        lhood1 = param1[self.num_param + 1]\n",
    "        w2 = param2[0:self.num_param]\n",
    "        w2 = self.gnn.dictfromlist(w2)\n",
    "        lhood2 = param2[self.num_param + 1]\n",
    "        T2 = param2[self.num_param + 2]\n",
    "        # SWAPPING PROBABILITIES\n",
    "        lhood12, dump1, dump2 = ptReplica.likelihood_func(self.gnn, self.traindata, w1, T2)\n",
    "        lhood21, dump1, dump2 = ptReplica.likelihood_func(self.gnn, self.traindata, w2, T1)\n",
    "        try:\n",
    "            swap_proposal = min(1, np.exp((lhood12 - lhood1) + (lhood21 - lhood2)))\n",
    "        except OverflowError:\n",
    "            swap_proposal = 1\n",
    "        u = np.random.uniform(0, 1)\n",
    "        if u < swap_proposal:\n",
    "            swapped = True\n",
    "            self.total_swap_proposals += 1\n",
    "            self.num_swap += 1\n",
    "            param_temp = param1\n",
    "            param1 = param2\n",
    "            param2 = param_temp\n",
    "            param1[self.num_param + 1] = lhood21\n",
    "            param2[self.num_param + 1] = lhood12\n",
    "            param1[self.num_param + 2] = T2\n",
    "            param2[self.num_param + 2] = T1\n",
    "        else:\n",
    "            swapped = False\n",
    "            self.total_swap_proposals += 1\n",
    "        return param1, param2, swapped\n",
    "\n",
    "    def run_chains(self):\n",
    "        # only adjacent chains can be swapped therefore, the number of proposals is ONE less num_chains\n",
    "        # swap_proposal = np.ones(self.num_chains-1)\n",
    "        # create parameter holders for paramaters that will be swapped\n",
    "        # replica_param = np.zeros((self.num_chains, self.num_param))\n",
    "        # lhood = np.zeros(self.num_chains)\n",
    "        # Define the starting and ending of MCMC Chains\n",
    "        start = 0\n",
    "        end = self.NumSamples - 1\n",
    "        # number_exchange = np.zeros(self.num_chains)\n",
    "        # filen = open(self.path + '/num_exchange.txt', 'a')\n",
    "        # RUN MCMC CHAINS\n",
    "        for l in range(0, self.num_chains):\n",
    "            self.chains[l].start_chain = start\n",
    "            self.chains[l].end = end\n",
    "        #start_time = time.time()\n",
    "        for j in range(0, self.num_chains):\n",
    "            self.wait_chain[j].clear()\n",
    "            self.event[j].clear()\n",
    "            self.chains[j].start()\n",
    "        # SWAP PROCEDURE\n",
    "        swaps_affected_main = 0\n",
    "        total_swaps = 0\n",
    "        for i in range(int(self.NumSamples / self.swap_interval)):\n",
    "            # print(i,int(self.NumSamples/self.swap_interval), 'Counting')\n",
    "            count = 0\n",
    "            for index in range(self.num_chains):\n",
    "                if not self.chains[index].is_alive():\n",
    "                    count += 1\n",
    "                    self.wait_chain[index].set()\n",
    "                    # print(str(self.chains[index].temperature) + \" Dead\" + str(index))\n",
    "\n",
    "            if count == self.num_chains:\n",
    "                break\n",
    "            # print(count,'Is the Count')\n",
    "            timeout_count = 0\n",
    "            for index in range(0, self.num_chains):\n",
    "                # print(\"Waiting for chain: {}\".format(index+1))\n",
    "                flag = self.wait_chain[index].wait()\n",
    "                if flag:\n",
    "                    # print(\"Signal from chain: {}\".format(index+1))\n",
    "                    timeout_count += 1\n",
    "\n",
    "            if timeout_count != self.num_chains:\n",
    "                # print(\"Skipping the Swap!\")\n",
    "                continue\n",
    "            # print(\"Event Occured\")\n",
    "\n",
    "            for index in range(0, self.num_chains - 1):\n",
    "                # print('Starting Swap')\n",
    "                swapped = False\n",
    "                param_1, param_2, swapped = self.swap_procedure(self.parameter_queue[index],\n",
    "                                                                self.parameter_queue[index + 1])\n",
    "                self.parameter_queue[index].put(param_1)\n",
    "                self.parameter_queue[index + 1].put(param_2)\n",
    "                if index == 0:\n",
    "                    if swapped:\n",
    "                        swaps_affected_main += 1\n",
    "                    total_swaps += 1\n",
    "            for index in range(self.num_chains):\n",
    "                self.wait_chain[index].clear()\n",
    "                self.event[index].set()\n",
    "            #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        print(\"Joining Processes\")\n",
    "\n",
    "        # JOIN THEM TO MAIN PROCESS\n",
    "        for index in range(0, self.num_chains):\n",
    "            print('Waiting to Join ', index, self.num_chains)\n",
    "            print(self.chains[index].is_alive())\n",
    "            self.chains[index].join()\n",
    "            print(index, 'Chain Joined')\n",
    "        self.chain_queue.join()\n",
    "        # pos_w, fx_train, fx_test, rmse_train, rmse_test, acc_train, acc_test, likelihood_vec, accept_vec, accept = self.show_results()\n",
    "        rmse_train, rmse_test, acc_train, acc_test, apal = self.show_results()\n",
    "        print(\"NUMBER OF SWAPS = \", self.num_swap)\n",
    "        swap_perc = self.num_swap * 100 / self.total_swap_proposals\n",
    "        # return pos_w, fx_train, fx_test, rmse_train, rmse_test, acc_train, acc_test, likelihood_vec, swap_perc, accept_vec, accept\n",
    "        return rmse_train, rmse_test, acc_train, acc_test, apal, swap_perc\n",
    "\n",
    "    def show_results(self):\n",
    "        burnin_samples = int(self.NumSamples * self.burn_in)\n",
    "        mcmc_samples = int(self.NumSamples * 0.25)\n",
    "        # likelihood_rep = np.zeros((self.num_chains, self.NumSamples - burnin,2))  # index 1 for likelihood posterior and index 0 for Likelihood proposals. Note all likilihood proposals plotted only\n",
    "        # accept_percent = np.zeros((self.num_chains, 1))\n",
    "        # accept_list = np.zeros((self.num_chains, self.NumSamples))\n",
    "        # pos_w = np.zeros((self.num_chains, self.NumSamples - burnin, self.num_param))\n",
    "        # fx_train_all = np.zeros((self.num_chains, self.NumSamples - burnin, len(self.traindata)))\n",
    "        rmse_train = np.zeros((self.num_chains, self.NumSamples))\n",
    "        acc_train = np.zeros((self.num_chains, self.NumSamples))\n",
    "\n",
    "        # fx_test_all = np.zeros((self.num_chains, self.NumSamples - burnin, len(self.testdata)))\n",
    "        rmse_test = np.zeros((self.num_chains, self.NumSamples))\n",
    "        acc_test = np.zeros((self.num_chains, self.NumSamples))\n",
    "        sum_val_array = np.zeros((self.num_chains, self.NumSamples))\n",
    "\n",
    "        weight_ar = np.zeros((self.num_chains, self.NumSamples))\n",
    "        weight_ar1 = np.zeros((self.num_chains, self.NumSamples))\n",
    "        weight_ar2 = np.zeros((self.num_chains, self.NumSamples))\n",
    "        weight_ar3 = np.zeros((self.num_chains, self.NumSamples))\n",
    "        weight_ar4 = np.zeros((self.num_chains, self.NumSamples))\n",
    "        likelihood_val_array = np.zeros((self.num_chains, self.NumSamples))\n",
    "\n",
    "        accept_percentage_all_chains = np.zeros(self.num_chains)\n",
    "\n",
    "        for i in range(self.num_chains):\n",
    "            # file_name = self.path + '/posterior/pos_w/' + 'chain_' + str(self.temperatures[i]) + '.txt'\n",
    "            # print(self.path)\n",
    "            # print(file_name)\n",
    "            # dat = np.loadtxt(file_name)\n",
    "            # pos_w[i, :, :] = dat[burnin:, :]\n",
    "\n",
    "            # file_name = self.path + '/posterior/pos_likelihood/' + 'chain_' + str(self.temperatures[i]) + '.txt'\n",
    "            # dat = np.loadtxt(file_name)\n",
    "            # likelihood_rep[i, :] = dat[burnin:]\n",
    "\n",
    "            # file_name = self.path + '/posterior/accept_list/' + 'chain_' + str(self.temperatures[i]) + '.txt'\n",
    "            # dat = np.loadtxt(file_name)\n",
    "            # accept_list[i, :] = dat\n",
    "\n",
    "            file_name = self.path + '/predictions/rmse_test_chain_' + str(self.temperatures[i]) + '.txt'\n",
    "            dat = np.loadtxt(file_name)\n",
    "            rmse_test[i, :] = dat\n",
    "\n",
    "            file_name = self.path + '/predictions/rmse_train_chain_' + str(self.temperatures[i]) + '.txt'\n",
    "            dat = np.loadtxt(file_name)\n",
    "            rmse_train[i, :] = dat\n",
    "\n",
    "            file_name = self.path + '/predictions/acc_test_chain_' + str(self.temperatures[i]) + '.txt'\n",
    "            dat = np.loadtxt(file_name)\n",
    "            acc_test[i, :] = dat\n",
    "\n",
    "            file_name = self.path + '/predictions/acc_train_chain_' + str(self.temperatures[i]) + '.txt'\n",
    "            dat = np.loadtxt(file_name)\n",
    "            acc_train[i, :] = dat\n",
    "\n",
    "            file_name = self.path + '/predictions/sum_value_' + str(self.temperatures[i]) + '.txt'\n",
    "            dat = np.loadtxt(file_name)\n",
    "            sum_val_array[i, :] = dat\n",
    "\n",
    "            file_name = self.path + '/predictions/weight[0]_' + str(self.temperatures[i]) + '.txt'\n",
    "            dat = np.loadtxt(file_name)\n",
    "            weight_ar[i, :] = dat\n",
    "\n",
    "            file_name = self.path + '/predictions/weight[100]_' + str(self.temperatures[i]) + '.txt'\n",
    "            dat = np.loadtxt(file_name)\n",
    "            weight_ar1[i, :] = dat\n",
    "\n",
    "            file_name = self.path + '/predictions/weight[1000]_' + str(self.temperatures[i]) + '.txt'\n",
    "            dat = np.loadtxt(file_name)\n",
    "            weight_ar2[i, :] = dat\n",
    "\n",
    "            file_name = self.path + '/predictions/weight[5000]_' + str(self.temperatures[i]) + '.txt'\n",
    "            dat = np.loadtxt(file_name)\n",
    "            weight_ar3[i, :] = dat\n",
    "\n",
    "            file_name = self.path + '/predictions/weight[8000]_' + str(self.temperatures[i]) + '.txt'\n",
    "            dat = np.loadtxt(file_name)\n",
    "            weight_ar4[i, :] = dat\n",
    "\n",
    "            file_name = self.path + '/predictions/accept_percentage' + str(self.temperatures[i]) + '.txt'\n",
    "            dat = np.loadtxt(file_name)\n",
    "            accept_percentage_all_chains[i] = dat\n",
    "\n",
    "            file_name = self.path + '/likelihood_value_' + str(self.temperatures[i]) + '.txt'\n",
    "            dat = np.loadtxt(file_name)\n",
    "            likelihood_val_array[i, :] = dat\n",
    "\n",
    "        rmse_train_single_chain_plot = rmse_train[0, :]\n",
    "        rmse_test_single_chain_plot = rmse_test[0, :]\n",
    "        acc_train_single_chain_plot = acc_train[0, :]\n",
    "        acc_test_single_chain_plot = acc_test[0, :]\n",
    "        sum_val_array_single_chain_plot = sum_val_array[0]\n",
    "        likelihood_val_array_single_chain_plot = likelihood_val_array[0]\n",
    "\n",
    "        #path = 'GNN/graphs'\n",
    "\n",
    "        x2 = np.linspace(0, self.NumSamples, num=self.NumSamples)\n",
    "\n",
    "        plt.plot(x2, sum_val_array_single_chain_plot, label='Sum Value')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.savefig(self.path + '/graphs/sum_value_single_chain.png')\n",
    "        plt.clf()\n",
    "\n",
    "        plt.plot(x2, likelihood_val_array_single_chain_plot, label='Sum Value')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.ylabel(\"Likelihood\", fontsize=13)\n",
    "        plt.xlabel(\"Samples\", fontsize=13)\n",
    "        plt.yticks(fontsize=13)\n",
    "        plt.xticks(fontsize=13)\n",
    "        plt.savefig(self.path + '/likelihood_value_single_chain.png')\n",
    "        plt.clf()\n",
    "\n",
    "        color = 'tab:red'\n",
    "        plt.plot(x2, acc_train_single_chain_plot, label=\"Train\", color=color)\n",
    "        color = 'tab:blue'\n",
    "        plt.plot(x2, acc_test_single_chain_plot, label=\"Test\", color=color)\n",
    "        plt.xlabel('Samples',fontsize=13)\n",
    "        plt.ylabel('Accuracy',fontsize=13)\n",
    "        plt.yticks(fontsize=13)\n",
    "        plt.xticks(fontsize=13)\n",
    "        plt.legend()\n",
    "        plt.savefig(self.path + '/graphs/superimposed_acc_single_chain.png')\n",
    "        plt.clf()\n",
    "\n",
    "        color = 'tab:red'\n",
    "        plt.plot(x2, rmse_train_single_chain_plot, label=\"Train\", color=color)\n",
    "        color = 'tab:blue'\n",
    "        plt.plot(x2, rmse_test_single_chain_plot, label=\"Test\", color=color)\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.savefig(self.path + '/graphs/superimposed_rmse_single_chain.png')\n",
    "        plt.clf()\n",
    "\n",
    "        rmse_train1 = rmse_train[:, burnin_samples:]\n",
    "        rmse_test1 = rmse_test[:, burnin_samples:]\n",
    "        acc_train1 = acc_train[:, burnin_samples:]\n",
    "        acc_test1 = acc_test[:, burnin_samples:]\n",
    "\n",
    "        rmse_train = rmse_train.reshape((self.num_chains * self.NumSamples), 1)\n",
    "        acc_train = acc_train.reshape((self.num_chains * self.NumSamples), 1)\n",
    "        rmse_test = rmse_test.reshape((self.num_chains * self.NumSamples), 1)\n",
    "        acc_test = acc_test.reshape((self.num_chains * self.NumSamples), 1)\n",
    "        sum_val_array = sum_val_array.reshape((self.num_chains * self.NumSamples), 1)\n",
    "        weight_ar = weight_ar.reshape((self.num_chains * self.NumSamples), 1)\n",
    "        weight_ar1 = weight_ar1.reshape((self.num_chains * self.NumSamples), 1)\n",
    "        weight_ar2 = weight_ar2.reshape((self.num_chains * self.NumSamples), 1)\n",
    "        weight_ar3 = weight_ar3.reshape((self.num_chains * self.NumSamples), 1)\n",
    "        weight_ar4 = weight_ar4.reshape((self.num_chains * self.NumSamples), 1)\n",
    "\n",
    "        x = np.linspace(0, int(self.masternumsample - self.masternumsample * self.burni),\n",
    "                        num=int(self.masternumsample - self.masternumsample * self.burni))\n",
    "        x1 = np.linspace(0, self.masternumsample, num=self.masternumsample)\n",
    "\n",
    "\n",
    "        plt.plot(x1, weight_ar, label='Weight[0]')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.ylabel('Parameter Values', fontsize=13)\n",
    "        plt.xlabel('Samples', fontsize=13)\n",
    "        plt.yticks(fontsize=13)\n",
    "        plt.xticks(fontsize=13)\n",
    "        plt.savefig(self.path + '/graphs/weight[0]_samples.png')\n",
    "        plt.clf()\n",
    "\n",
    "        plt.hist(weight_ar, bins=20, color=\"blue\", alpha=0.7)\n",
    "        plt.ylabel('Frequency', fontsize=13)\n",
    "        plt.xlabel('Parameter Values', fontsize=13)\n",
    "        plt.yticks(fontsize=13)\n",
    "        plt.xticks(fontsize=13)\n",
    "        plt.savefig(self.path + '/graphs/weight[0]_hist.png')\n",
    "        plt.clf()\n",
    "\n",
    "        plt.plot(x1, weight_ar1, label='Weight[100]')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.ylabel('Parameter Values', fontsize=13)\n",
    "        plt.xlabel('Samples', fontsize=13)\n",
    "        plt.yticks(fontsize=13)\n",
    "        plt.xticks(fontsize=13)\n",
    "        plt.savefig(self.path + '/graphs/weight[100]_samples.png')\n",
    "        plt.clf()\n",
    "\n",
    "        plt.hist(weight_ar1, bins=20, color=\"blue\", alpha=0.7)\n",
    "        plt.ylabel('Frequency', fontsize=13)\n",
    "        plt.xlabel('Parameter Values', fontsize=13)\n",
    "        plt.yticks(fontsize=13)\n",
    "        plt.xticks(fontsize=13)\n",
    "        plt.savefig(self.path + '/graphs/weight[100]_hist.png')\n",
    "        plt.clf()\n",
    "\n",
    "        plt.plot(x1, weight_ar2, label='Weight[1000]')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.ylabel('Parameter Values', fontsize=13)\n",
    "        plt.xlabel('Samples', fontsize=13)\n",
    "        plt.yticks(fontsize=13)\n",
    "        plt.xticks(fontsize=13)\n",
    "        plt.savefig(self.path + '/graphs/weight[1000]_samples.png')\n",
    "        plt.clf()\n",
    "\n",
    "        plt.hist(weight_ar2, bins=20, color=\"blue\", alpha=0.7)\n",
    "        plt.ylabel('Frequency', fontsize=13)\n",
    "        plt.xlabel('Parameter Values', fontsize=13)\n",
    "        plt.yticks(fontsize=13)\n",
    "        plt.xticks(fontsize=13)\n",
    "        plt.savefig(self.path + '/graphs/weight[1000]_hist.png')\n",
    "        plt.clf()\n",
    "\n",
    "        plt.plot(x1, weight_ar3, label='Weight[5000]')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.ylabel('Parameter Values', fontsize=13)\n",
    "        plt.xlabel('Samples', fontsize=13)\n",
    "        plt.yticks(fontsize=13)\n",
    "        plt.xticks(fontsize=13)\n",
    "        plt.savefig(self.path + '/graphs/weight[5000]_samples.png')\n",
    "        plt.clf()\n",
    "\n",
    "        plt.hist(weight_ar3, bins=20, color=\"blue\", alpha=0.7)\n",
    "        plt.ylabel('Frequency', fontsize=13)\n",
    "        plt.xlabel('Parameter Values', fontsize=13)\n",
    "        plt.yticks(fontsize=13)\n",
    "        plt.xticks(fontsize=13)\n",
    "        plt.savefig(self.path + '/graphs/weight[5000]_hist.png')\n",
    "        plt.clf()\n",
    "\n",
    "        plt.plot(x1, weight_ar4, label='Weight[8000]')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.ylabel('Parameter Values', fontsize=13)\n",
    "        plt.xlabel('Samples', fontsize=13)\n",
    "        plt.yticks(fontsize=13)\n",
    "        plt.xticks(fontsize=13)\n",
    "        plt.savefig(self.path + '/graphs/weight[8000]_samples.png')\n",
    "        plt.clf()\n",
    "\n",
    "        plt.hist(weight_ar4, bins=20, color=\"blue\", alpha=0.7)\n",
    "        plt.ylabel('Frequency', fontsize=13)\n",
    "        plt.xlabel('Parameter Values', fontsize=13)\n",
    "        plt.yticks(fontsize=13)\n",
    "        plt.xticks(fontsize=13)\n",
    "        plt.savefig(self.path + '/graphs/weight[8000]_hist.png')\n",
    "        plt.clf()\n",
    "\n",
    "        plt.plot(x1, sum_val_array, label='Sum_Value')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(\"Sum Value Over Samples\")\n",
    "        plt.savefig(self.path + '/graphs/sum_value_samples.png')\n",
    "        plt.clf()\n",
    "\n",
    "        color = 'tab:red'\n",
    "        plt.plot(x1, acc_train, label=\"Train\", color=color)\n",
    "        color = 'tab:blue'\n",
    "        plt.plot(x1, acc_test, label=\"Test\", color=color)\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.savefig(self.path + '/graphs/superimposed_acc.png')\n",
    "        plt.clf()\n",
    "\n",
    "        color = 'tab:red'\n",
    "        plt.plot(x1, rmse_train, label=\"Train\", color=color)\n",
    "        color = 'tab:blue'\n",
    "        plt.plot(x1, rmse_test, label=\"Test\", color=color)\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.savefig(self.path + '/graphs/superimposed_rmse.png')\n",
    "        plt.clf()\n",
    "\n",
    "        return rmse_train1, rmse_test1, acc_train1, acc_test1, accept_percentage_all_chains\n",
    "\n",
    "    def make_directory(self, directory):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    numSamples = args.samples\n",
    "    num_chains = args.num_chains\n",
    "    burn_in = args.burn_in\n",
    "    learning_rate = args.learning_rate\n",
    "    step_size = args.step_size\n",
    "    maxtemp = 2\n",
    "    use_langevin_gradients = True  # False leaves it as Random-walk proposals. Note that Langevin gradients will take a bit more time computationally\n",
    "    bi = burn_in\n",
    "    swap_interval = 2 #how ofen you swap neighbours. note if swap is more than Num_samples, its off\n",
    "\n",
    "    # learn_rate = 0.01  # in case langevin gradients are used. Can select other values, we found small value is ok.\n",
    "\n",
    "    problemfolder = 'Graph_torch/GNN'  # change this to your directory for results output - produces large datasets\n",
    "\n",
    "    name = \"\"\n",
    "    filename = \"\"\n",
    "\n",
    "    if not os.path.exists(problemfolder + name):\n",
    "        os.makedirs(problemfolder + name)\n",
    "    path = (problemfolder + name)\n",
    "\n",
    "    timer = time.time()\n",
    "\n",
    "    pt = ParallelTempering(use_langevin_gradients, learning_rate, num_chains, maxtemp, numSamples,\n",
    "                           swap_interval, path, bi, step_size)\n",
    "\n",
    "    directories = [path + '/predictions/', path + '/graphs/']\n",
    "    for d in directories:\n",
    "        pt.make_directory((filename) + d)\n",
    "\n",
    "    pt.initialize_chains(burn_in)\n",
    "    # pos_w, fx_train, fx_test, rmse_train, rmse_test, acc_train, acc_test, likelihood_rep, swap_perc, accept_vec, accept = pt.run_chains()\n",
    "    rmse_train, rmse_test, acc_train, acc_test, accept_percent_all, sp = pt.run_chains()\n",
    "\n",
    "    timer2 = time.time()\n",
    "\n",
    "    # list_end = accept_vec.shape[1]\n",
    "    # accept_ratio = accept_vec[:,  list_end-1:list_end]/list_end\n",
    "    # accept_per = np.mean(accept_ratio) * 100\n",
    "    # print(accept_per, ' accept_per')\n",
    "\n",
    "    timetotal = (timer2 - timer) / 60\n",
    "\n",
    "    \"\"\"\n",
    "    # #PLOTS\n",
    "    acc_tr = np.mean(acc_train [:])\n",
    "    acctr_std = np.std(acc_train[:])\n",
    "    acctr_max = np.amax(acc_train[:])\n",
    "    acc_tes = np.mean(acc_test[:])\n",
    "    acctest_std = np.std(acc_test[:])\n",
    "    acctes_max = np.amax(acc_test[:])\n",
    "    rmse_tr = np.mean(rmse_train[:])\n",
    "    rmsetr_std = np.std(rmse_train[:])\n",
    "    rmsetr_max = np.amax(acc_train[:])\n",
    "    rmse_tes = np.mean(rmse_test[:])\n",
    "    rmsetest_std = np.std(rmse_test[:])\n",
    "    rmsetes_max = np.amax(rmse_test[:])\n",
    "    \"\"\"\n",
    "\n",
    "    rmse_tr = np.mean(rmse_train)\n",
    "    rmsetr_std = np.std(rmse_train)\n",
    "    rmsetr_max = np.amin(rmse_train)\n",
    "\n",
    "    rmse_tes = np.mean(rmse_test)\n",
    "    rmsetest_std = np.std(rmse_test)\n",
    "    rmsetes_max = np.amin(rmse_test)\n",
    "\n",
    "    acc_tr = np.mean(acc_train)\n",
    "    acctr_std = np.std(acc_train)\n",
    "    acctr_max = np.amax(acc_train)\n",
    "\n",
    "    acc_tes = np.mean(acc_test)\n",
    "    acctest_std = np.std(acc_test)\n",
    "    acctes_max = np.amax(acc_test)\n",
    "\n",
    "    accept_percent_mean = np.mean(accept_percent_all)\n",
    "\n",
    "    # outres = open(path+'/result.txt', \"a+\")\n",
    "    # outres_db = open(path_db+'/result.txt', \"a+\")\n",
    "    # resultingfile = open(problemfolder+'/master_result_file.txt','a+')\n",
    "    # resultingfile_db = open( problemfolder_db+'/master_result_file.txt','a+')\n",
    "    # xv = name+'_'+ str(run_nb)\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    print(\"Train Acc (Mean, Max, Std)\")\n",
    "    print(acc_tr, acctr_max, acctr_std)\n",
    "    print(\"\\n\")\n",
    "    print(\"Test Acc (Mean, Max, Std)\")\n",
    "    print(acc_tes, acctes_max, acctest_std)\n",
    "    print(\"\\n\")\n",
    "    print(\"Train RMSE (Mean, Max, Std)\")\n",
    "    print(rmse_tr, rmsetr_max, rmsetr_std)\n",
    "    print(\"\\n\")\n",
    "    print(\"Test RMSE (Mean, Max, Std)\")\n",
    "    print(rmse_tes, rmsetes_max, rmsetest_std)\n",
    "    print(\"\\n\")\n",
    "    print(\"Acceptance Percentage Mean\")\n",
    "    print(accept_percent_mean)\n",
    "    print(\"\\n\")\n",
    "    print(\"Swap Percentage\")\n",
    "    print(sp)\n",
    "    print(\"\\n\")\n",
    "    print(\"Time (Minutes)\")\n",
    "    print(timetotal)\n",
    "\n",
    "if __name__ == \"__main__\": main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1cb4ba5f411cfa4a68a7ea6c2f9ba3655e2604bd37447d058a856eda531fd15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
